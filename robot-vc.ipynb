{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import join as opj "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure TRIAAN-VC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "ORIGINAL_MODEL_PATH = opj(os.getcwd(), 'models', 'triaanvc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tested with Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters List\n",
    "- `--config` : direct of configuration file \n",
    "- `--device` : device to run the conversion on (default: `cuda:0`)\n",
    "- `--sample_path`: path to the sample data\n",
    "- `--src_name`: name of the source data \n",
    "- `--trg_name`: name of the target data\n",
    "- `--checkpoints`: path to the checkpoints directory\n",
    "- `--model_name`: name of the model, injecting with the custom pre-trained path\n",
    "- `--seed`: seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure path to the pretrained model\n",
    "PRETRAINED_TRIAAN = os.path.join(os.getcwd(), 'pre-trained','triaan-vc')\n",
    "### others pretrained models: CPC, Vocoder -------------------\n",
    "PRETRAINED_CPC = os.path.join(os.getcwd(), 'pre-trained','cpc')\n",
    "PRETRAINED_Vocoder = os.path.join(os.getcwd(), 'pre-trained','vocoder')\n",
    "\n",
    "print(PRETRAINED_TRIAAN)\n",
    "print(PRETRAINED_CPC)\n",
    "print(PRETRAINED_Vocoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Folder configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to model configuration\n",
    "convert_file = os.path.join(ORIGINAL_TRIAAN, 'convert.py')\n",
    "original_config = os.path.join(ORIGINAL_TRIAAN, 'config', 'base.yaml')\n",
    "triaan_model = os.path.join(PRETRAINED_TRIAAN, 'model-cpc-split.pth') \n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "data_source = './data'\n",
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "print(convert_file)\n",
    "print(original_config)\n",
    "print(triaan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source and target audio files\n",
    "source_name = 'MONTREAL_6_happiness_m.wav'\n",
    "target_name = 'ken8hapvoc.wav'\n",
    "\n",
    "# source_name = 'film_female_1.mp3'\n",
    "# target_name = 'film_male_1.mp3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the original model to the pre-trained model\n",
    "#os.system(f'python {convert_file} --device {device} --config {original_config} --sample_path {data_source} --src_name {source_name} --trg_name {target_name} --checkpoint {checkpoint_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python  --device cpu --sample_path ./data --src_name MONTREAL_6_happiness_m.wav --trg_name ken8hapvoc.wav  --checkpoint ./checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Original model and fine-tune with custom data for specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/winddori2002/TriAAN-VC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In preporcessing phase (`audio.py`)file, normalise the pitch and loudness of the audio files \n",
    "- Recreate the preprocess configuration file (with customise data - robotic data)\n",
    "- Dataset that we need to use: `WILLOW, BEST (Possible EMOGIB)`\n",
    "- Train the model with the new configuration\n",
    "- Save the new model\n",
    "- Evaluate the model ~ WER, CER, SV Average\n",
    "  \n",
    "\n",
    "**Original dataset:** VCTK - 110 speakers with 400 utterances per speaker.\n",
    "\n",
    "***Custom dataset:*** Robotic data\n",
    "\n",
    "**Expected  Output**: The model can be use to convert the human speaking voice into robotic voice and it also encapsulating all the pause, and different in timbre. Source input will be human voice, target input will be robotic voice and the output is the conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO: Rewrite all the functions to match with custom data. We only keep the model architecture, but the data processing, training, and evaluation will be re-written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyyaml\n",
    "from modified_audio_process import * #import all the functions changes for preprocessing audio\n",
    "from models.triaanvc.src.utils import Config\n",
    "\n",
    "#import preprocess, convert, train and test functions\n",
    "# from models.triaanvc.preprocess_cpc import main as model_preprocess_cpc\n",
    "# from models.triaanvc.preprocess import main as model_preprocess\n",
    "# from models.triaanvc.convert import main as model_convert\n",
    "# from models.triaanvc.train import main as model_train\n",
    "# from models.triaanvc.test import main as model_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cfg = Config('./custom-config/preprocess.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triaan_preprocess():\n",
    "    # model_preprocess\n",
    "    pass\n",
    "\n",
    "# Function to execute the train or test in the TRIAAN-VC model, with specify parameters:\n",
    "# action # train / test\n",
    "# --config # path to config file\n",
    "# --num_worker # number of workers\n",
    "# --seed # seed number\n",
    "# --device # cuda device - cpu or device\n",
    "# --logging # logging option\n",
    "# --resume # resume option\n",
    "# --checkpoint # results save path\n",
    "# --model_name # best model name\n",
    "# --n_uttr # number of target utterances (default 1)\n",
    "def get_triaan_execute():\n",
    "    # using model_train or model_test\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute the convert.py file in the TRIAAN-VC model, with specify parameters\n",
    "def get_triaan_convert():\n",
    "    # using model_convert\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Input\n",
    "- reduce amplitude of target (robot) to match amplitude values of source (human)\n",
    "- lower pitch of robot to human range (find out human range)\n",
    "- remove the breathing/silence parts from source\n",
    "- adding small silence at the start and at the end of the robot audio file\n",
    "or combine 2 robot audios (from same source/similar sounding) with a silence gap in between them (maybe trying with samples from R2 or Wall-E since it's easier to find samples)\n",
    "- play with combinations of the things above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import join as opj \n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A multiple processes function\n",
    "def input_process(source_path, target_path, processed_path, sr=16000, top_db=60):\n",
    "    # function that fully process the input audio files from source and target \n",
    "\n",
    "    src_wav, src_fs  = sf.read(source_path)\n",
    "    tgt_wav, tgt_fs   = sf.read(target_path)\n",
    "\n",
    "    # trim slience\n",
    "    src_wav, _   = librosa.effects.trim(y=wav, top_db=top_db)\n",
    "    tgt_wav, _   = librosa.effects.trim(y=wav, top_db=top_db)\n",
    "\n",
    "    # resample\n",
    "    if src_fs != sr:\n",
    "        src_wav = resampy.resample(x=src_wav, sr_orig=fs, sr_new=sr, axis=0)\n",
    "        fs  = sr\n",
    "    if tgt_fs != sr:\n",
    "        tgt_wav = resampy.resample(x=tgt_wav, sr_orig=fs, sr_new=sr, axis=0)\n",
    "        fs  = sr\n",
    "\n",
    "    peak_src_max = np.abs(src_wav).max()\n",
    "    peak_src_min = np.abs(src_wav).min()\n",
    "\n",
    "    peak_tgt_max = np.abs(tgt_wav).max()\n",
    "    peak_tgt_min = np.abs(tgt_wav).min()\n",
    "\n",
    "    # normalise amplitude of tgt_wav to match src_wav\n",
    "    if peak_tgt_max > peak_src_max:\n",
    "        tgt_wav /= peak_src_max\n",
    "    elif peak_tgt_min < peak_src_min:\n",
    "        tgt_wav *= peak_src_min / peak_tgt_min\n",
    "\n",
    "\n",
    "    # lower pitch of the target audio (robot) to human pitch range\n",
    "    tgt_wav = librosa.effects.pitch_shift(tgt_wav, sr, n_steps=-2)\n",
    "\n",
    "    # save the processed audio files\n",
    "    src_name = source_path.split('/')[-1].split('.')[0]\n",
    "    src_name = target_path.split('/')[-1].split('.')[0]\n",
    "    sf.write(opj(processed_path, f's_processed_{src_name}.wav'), src_wav, sr)\n",
    "    sf.write(opj(processed_path, f't_processed_{tgt_name}.wav'), tgt_wav, sr)\n",
    "\n",
    "    return src_wav, tgt_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate Processed Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_audio(audio, fs, sr=16000):\n",
    "    if fs != sr:\n",
    "        audio = resampy.resample(x=audio, sr_orig=fs, sr_new=sr, axis=0)\n",
    "        fs  = sr\n",
    "    return audio, fs\n",
    "\n",
    "def trim_silence(audio, top_db=60):\n",
    "    audio, _   = librosa.effects.trim(y=audio, top_db=top_db)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_processed_audio(output_path, audio_path, processed_audio, fs, tag='fullprocessed'):\n",
    "    name = audio_path.split('\\\\')[-1].split('.')[0]\n",
    "    sf.write(opj(output_path, f'{tag}_{name}.wav'), processed_audio, fs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acoustic Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_audio_files(audio1_path, audio2_path, processed_path):\n",
    "    \"\"\"\n",
    "    Concatenate two audio files without silence in between \n",
    "    and save the concatenated audio file in processed_path\n",
    "    \"\"\"\n",
    "    # function that concatenate two audio files\n",
    "    # and save the concatenated audio file in processed_path\n",
    "    audio1, fs1 = sf.read(audio1_path)\n",
    "    audio2, fs2 = sf.read(audio2_path)\n",
    "\n",
    "    # resample both audio files to 16kHz\n",
    "    audio1, fs1 = resample_audio(audio1, fs1)\n",
    "    audio2, fs2 = resample_audio(audio2, fs2)\n",
    "\n",
    "    # concatenate two audio files\n",
    "    concatenated_audio = np.concatenate((audio1, audio2), axis=0)\n",
    "    \n",
    "\n",
    "    audio1_name = audio1_path.split('\\\\')[-1].split('.')[0]\n",
    "    audio2_name = audio2_path.split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    sf.write(opj(processed_path, f'concatenated_{audio1_name}_{audio2_name}.wav'), concatenated_audio, fs1)\n",
    "\n",
    "    return concatenated_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_amplitude(audio1_path, audio2_path, processed_path):\n",
    "    # function that match amplitude of audio2 to audio1\n",
    "    audio1, fs1 = sf.read(audio1_path) #human reading\n",
    "    audio2, fs2 = sf.read(audio2_path) #robot voice\n",
    "    # resample\n",
    "    audio1, fs1 = resample_audio(audio1, fs1)\n",
    "    audio2, fs2 = resample_audio(audio2, fs2)\n",
    "\n",
    "    # trim silence\n",
    "    audio1 = trim_silence(audio1)\n",
    "    audio2 = trim_silence(audio2)\n",
    "\n",
    "    \n",
    "    # normalise audio2 amplitude\n",
    "    audio2 = audio2 / np.abs(audio2).max() * np.abs(audio1).max()\n",
    "\n",
    "    # peak_audio1_max = np.abs(audio1).max()\n",
    "    # peak_audio1_min = np.abs(audio1).min()\n",
    "\n",
    "    # peak_audio2_max = np.abs(audio2).max()\n",
    "    # peak_audio2_min = np.abs(audio2).min()\n",
    "\n",
    "    # # normalise amplitude of audio2 to match audio1\n",
    "    # if peak_audio2_max > peak_audio1_max:\n",
    "    #     audio2 /= peak_audio1_max\n",
    "    # elif peak_audio2_min < peak_audio1_min:\n",
    "    #     audio2 *= peak_audio1_min / peak_audio2_min\n",
    "    # # elif peak_audio1_min < peak_audio2_min:\n",
    "    #     audio2 *= peak_audio2_min / peak_audio1_min\n",
    "    # outputing\n",
    "    output_processed_audio(processed_path, audio1_path, audio1, fs1, tag='normamplitude1')\n",
    "    output_processed_audio(processed_path, audio2_path, audio2, fs2, tag='normamplitude2')\n",
    "\n",
    "    return audio1, audio2 #audio 2 is normalised to amplitude of audio 1\n",
    "\n",
    "def pitch_shift(audio_path, processed_path, sr=16000, n_steps=-2):\n",
    "    \"\"\"\n",
    "    Shift pitch of an audio by n_steps\n",
    "    \"\"\"\n",
    "    audio, fs = sf.read(audio_path)\n",
    "    audio, fs = resample_audio(audio, fs, sr) # resample before pitch shift\n",
    "\n",
    "    audio = librosa.effects.pitch_shift(audio, fs, n_steps=n_steps)\n",
    "    output_processed_audio(processed_path, audio_path, audio, fs=fs, tag='pitchshift')\n",
    "    return audio\n",
    "\n",
    "def adjust_pitch(audio_path, pitch_range_start=90, pitch_range_end=155):\n",
    "    \"\"\"\n",
    "    Adjust pitch of an audio to a given range\n",
    "    by default, range is set to \n",
    "    (90 - 155) - adult male voice range or \n",
    "    (165 - 255) - adult female voice range\n",
    "    \"\"\"\n",
    "    audio, fs = sf.read(audio_path)\n",
    "\n",
    "    #resample before adjust pitch\n",
    "    audio, fs = resample_audio(audio, fs)\n",
    "\n",
    "    #calculate highest pitch\n",
    "    pitches, magnitudes = librosa.piptrack(y=audio, sr=fs)\n",
    "    max_pitch = pitches[np.argmax(magnitudes)]\n",
    "\n",
    "    # desire pitch from range \n",
    "    desired_pitch = np.random.uniform(pitch_range_start, pitch_range_end)\n",
    "    # n_steps = librosa.hz_to_octs(desired_pitch) - librosa.hz_to_octs(max_pitch)\n",
    "    n_steps = np.abs(librosa.hz_to_octs(desired_pitch) - librosa.hz_to_octs(max_pitch))\n",
    "    \n",
    "    normpitch_audio = librosa.effects.pitch_shift(audio, fs, n_steps)\n",
    "    output_processed_audio(processed_path, audio_path, normpitch_audio, fs=fs, tag='adjustpitch')\n",
    "\n",
    "    return normpitch_audio\n",
    "\n",
    "def add_silence(audio_path, processed_path, duration=0.5):\n",
    "    \"\"\"\n",
    "    Add silence to the beginning and end of the audio\n",
    "    \"\"\"\n",
    "    audio, fs = sf.read(audio_path)\n",
    "    audio, fs = resample_audio(audio, fs) # resample before adding silence, expected 16kHz\n",
    "\n",
    "    silence = np.zeros(int(duration * fs))\n",
    "    audio = np.concatenate((silence, audio, silence), axis=0)\n",
    "\n",
    "    output_processed_audio(processed_path, audio_path, audio, fs=fs, tag='addsilence')\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX PATH\n",
    "input_path = '.\\data\\Phuoc\\\\testing_conversion\\input'\n",
    "output_path = '.\\data\\Phuoc\\\\testing_conversion\\output'\n",
    "\n",
    "processed_path = '.\\data\\Phuoc\\\\testing_conversion\\processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "human1_path = opj(input_path, 'manreading1.wav') #human voice\n",
    "robot1_path = opj(input_path, 'best1137surprise.wav') #robot-voice 1\n",
    "robot2_path = opj(input_path, 'best1138surprise.wav') #same robot-voice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1** | Trim silence from both audios and matching the amplitude of target audio to source audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#matching amplitude of robot1 to human1\n",
    "human1, robot1 = matching_amplitude(human1_path, robot1_path, processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2**\n",
    "| Lower pitch of robot to human range (target audio into human pitch range)\n",
    "\n",
    "NOTES:\n",
    "- Male voice pitch range: 90 - 155Hz\n",
    "- Female voice pitch range: 165 - 255Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 22856 is out of bounds for axis 0 with size 1025",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m robot1 \u001b[38;5;241m=\u001b[39m \u001b[43madjust_pitch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrobot1_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 56\u001b[0m, in \u001b[0;36madjust_pitch\u001b[1;34m(audio_path, pitch_range_start, pitch_range_end)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#calculate highest pitch\u001b[39;00m\n\u001b[0;32m     55\u001b[0m pitches, magnitudes \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mpiptrack(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39mfs)\n\u001b[1;32m---> 56\u001b[0m max_pitch \u001b[38;5;241m=\u001b[39m \u001b[43mpitches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmagnitudes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# desire pitch from range \u001b[39;00m\n\u001b[0;32m     59\u001b[0m desired_pitch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(pitch_range_start, pitch_range_end)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 22856 is out of bounds for axis 0 with size 1025"
     ]
    }
   ],
   "source": [
    "robot1 = adjust_pitch(robot1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 3** | Adding small silence at the start and at the end of the robot audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot1 = add_silence(robot1_path, processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 4** | Concatenating 2 robot audios with a silence gap in between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.35169214e-06, -8.04107549e-06,  1.46622250e-05, ...,\n",
       "        8.36818821e-06,  1.36421141e-05, -3.64212635e-06])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_audio_files(robot1_path, robot2_path, processed_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voiceconversion-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
